{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "seq_len = 12\n",
    "dim = 5 # head_dim\n",
    "Q = torch.randn((seq_len, dim),requires_grad=True)\n",
    "K = torch.randn((seq_len, dim),requires_grad=True)\n",
    "V = torch.randn((seq_len, dim),requires_grad=True)\n",
    "S = torch.matmul(Q, K.transpose(0, 1)) # flops--> seq_len^2 x dim\n",
    "S.retain_grad()\n",
    "P = torch.softmax(S, dim=-1) # flops--> seq_len^2 x 2     (exp 一次，除sum一次)\n",
    "P.retain_grad()\n",
    "O_ori = torch.matmul(P, V) # flops--> seq_len^2 x dim\n",
    "O_ori.retain_grad()\n",
    "factor =  torch.randn_like(O_ori)\n",
    "# loss = torch.sum(O_ori * factor)\n",
    "# loss.backward(retain_graph=True)\n",
    "O_ori.backward(factor, retain_graph=True)\n",
    "dO = factor\n",
    "print(torch.all(O_ori.grad == dO))\n",
    "M_ori =  torch.max(S,dim=-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " tensor(5.5511e-16, grad_fn=<MaxBackward1>),\n",
       " tensor([4.4392, 2.9770, 5.4098, 3.5339, 3.2394, 2.7365, 3.3584, 4.1289, 4.4451,\n",
       "         3.5642, 3.7554, 3.9380], grad_fn=<CopySlices>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_qo= 2\n",
    "block_kv = 3\n",
    "O = torch.zeros_like(Q)\n",
    "M = torch.zeros(seq_len) # 保存max + ln^l\n",
    "for block_idx in range(seq_len // block_qo): # 按块遍历 Q\n",
    "    q = Q[block_idx*block_qo:(block_idx+1)*block_qo, :]\n",
    "    o = O[block_idx*block_qo:(block_idx+1)*block_qo, :]\n",
    "    m_i= torch.ones(block_qo)*float('-inf')\n",
    "    l_i = torch.zeros(block_qo)\n",
    "    for block_kv_idx in range(seq_len // block_kv): # 按块遍历 K, V\n",
    "        k = K[block_kv_idx*block_kv:(block_kv_idx+1)*block_kv, :]\n",
    "        v = V[block_kv_idx*block_kv:(block_kv_idx+1)*block_kv, :]\n",
    "\n",
    "        # 求局部值\n",
    "        s =torch.matmul(q, k.transpose(0, 1))\n",
    "        m_ij = torch.maximum(torch.max(s,dim=-1)[0],m_i) \n",
    "        # torch.max 返回最大值[0]和最大值的索引[1]  ,求出包含当前处理bolck_j的局部最大值\n",
    "        P_hat = torch.exp(s - m_ij[:,None]) # exp(s - max(s))\n",
    "        l_ij = torch.sum(P_hat, dim=-1)\n",
    "        alpha = torch.exp(m_i - m_ij)\n",
    "\n",
    "        # 更新\n",
    "        o[...] = o * alpha[:,None]\n",
    "        o[...] += torch.matmul(P_hat, v)\n",
    "        l_i = alpha*l_i + l_ij # 更新\n",
    "        m_i = m_ij  #更新\n",
    "\n",
    "\n",
    "    o[:] /= l_i[:,None]\n",
    "    M[block_idx*block_qo:(block_idx+1)*block_qo] = m_i + torch.log(l_i)\n",
    "\n",
    "\n",
    "torch.allclose(O,O_ori),torch.max(O-O_ori),M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S = QK^T \\in \\mathbb{R}^{N\\times N}$,   \n",
    "$P = sfotmax(S)\\in \\mathbb{R}^{N\\times N}$,        \n",
    "$O =PV \\in \\mathbb{R}^{N\\times d} $      \n",
    "\n",
    "记$\\frac{\\partial L_{loss}}{\\partial O} = dO$\n",
    "\n",
    "$ dV = P^T dO$,\n",
    "$ dP = dO V^T$,\n",
    "\n",
    "\n",
    "\n",
    "$ dP_{ij} = do_i^T v_j$\n",
    "\n",
    "$$\n",
    "dP_{i:} =\n",
    "\\begin{bmatrix}\n",
    "P_{i1} \\\\ P_{i2} \\\\ P_{i3} \\\\ ...\\\\ P_{ij} \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "do_i^T v_1 \\\\ do_i^T v_2 \\\\ do_i^T v_3 \\\\ ...\\\\ do_i^T v_j \\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "v_1^T do_i \\\\ v_2^T do_i \\\\ v_3^T do_i \\\\ ...\\\\ v_j^T do_i \\\\ \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "v_1^T  \\\\ v_2^T  \\\\ v_3^T  \\\\ ...\\\\ v_j^T \\\\ \n",
    "\\end{bmatrix} \\,do_i\n",
    "$$\n",
    "<!--  -->\n",
    "$$ \n",
    "\\begin{align}\n",
    "dS_{i:} &= M_{Jocabian}\\, dP_{i:} \\\\\n",
    "&= (diag(P_{i:}) - P_{i:} P_{i:}^T)dP_{i:}\\\\    \n",
    "&= diag(P_{i:})dP_{i:} - P_{i:} P_{i:}^TdP_{i:}\\\\\n",
    "&= P_{i:}\\circ dP_{i:} - P_{i:}^TdP_{i:}P_{i:} \\quad \\text{点积结果是标量可以交换}\n",
    "\\end{align}\n",
    "$$\n",
    "<!--  -->\n",
    "$$\n",
    "D_i = P_{i:}^TdP_{i:} \n",
    "= \n",
    "\\begin{bmatrix}\n",
    "P_{i1} &  P_{i2} & P_{i3} & ...& P_{ij} \n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "v_1^T  \\\\ v_2^T  \\\\ v_3^T  \\\\ ...\\\\ v_j^T \\\\ \n",
    "\\end{bmatrix} \\,do_i\n",
    "= o_i^T do_i = do_i^T o_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 该伪码实现参照flashattention v2\n",
    "\n",
    "block_qo= 2\n",
    "block_kv = 3\n",
    "\n",
    "dQ = torch.zeros_like(Q)\n",
    "dK = torch.zeros_like(K)\n",
    "dV = torch.zeros_like(V)\n",
    "D = torch.sum(dO * O,dim=-1)\n",
    "for block_kv_idx in range(seq_len//block_kv):\n",
    "    k_j = K[block_kv_idx*block_kv:(block_kv_idx+1)*block_kv,:]#load to SRAM\n",
    "    v_j = V[block_kv_idx*block_kv:(block_kv_idx+1)*block_kv,:]#load to SRAM\n",
    "    dk_j = torch.zeros_like(k_j)#init on SRAM\n",
    "    dv_j = torch.zeros_like(v_j)#init on SRAM\n",
    "    for block_qo_idx in range(seq_len//block_qo):\n",
    "        q_i = Q[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo,:] #load to SRAM\n",
    "        \n",
    "        o_i = O[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo,:] #load to SRAM\n",
    "        do_i = dO[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo,:] #load to SRAM\n",
    "        m_i = M[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo] #load to SRAM\n",
    "        D_i = D[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo] #load to SRAM\n",
    "\n",
    "        s_ij = torch.matmul(q_i, k_j.transpose(0, 1))\n",
    "        p_ij = torch.exp(s_ij-m_i[:,None])\n",
    "\n",
    "        dv_j += torch.matmul(p_ij.transpose(0, 1), do_i)\n",
    "        dp_ij = do_i @ v_j.transpose(0, 1)\n",
    "\n",
    "        ds_ij = p_ij * (dp_ij -D_i[:,None])\n",
    "\n",
    "        dq_i = dQ[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo,:] #load to SRAM\n",
    "        dq_i += torch.matmul(ds_ij, k_j)\n",
    "        dQ[block_qo_idx*block_qo:(block_qo_idx+1)*block_qo,:] = dq_i #write back to HBM\n",
    "\n",
    "        dk_j += torch.matmul(ds_ij.transpose(0, 1), q_i)\n",
    "\n",
    "    dK[block_kv_idx*block_kv:(block_kv_idx+1)*block_kv,:] = dk_j #write back to HBM\n",
    "    dV[block_kv_idx*block_kv:(block_kv_idx+1)*block_kv,:] = dv_j #write back to HBM\n",
    "\n",
    "torch.allclose(dQ,Q.grad),torch.allclose(dK,K.grad),torch.allclose(dV,V.grad)        \n",
    "# torch.max(dQ-Q.grad),torch.max(dK-K.grad),torch.max(dV-V.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([12, 5]) torch.Size([12, 5])\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dV = P.T @ dO  # seq_len x seq_len  @ seq_len x dim   flops--> seq_len^2 x dim\n",
    "print(torch.allclose(V.grad ,dV))\n",
    "dP = dO @ V.T # seq_len x dim @ dim x seq_len   flops--> seq_len^2 x dim\n",
    "print(torch.allclose(P.grad ,dP))\n",
    "D = torch.sum(dO * O_ori, dim=-1)\n",
    "dS = P * (dP - D[:, None]) # seq_len x seq_len     flops--> seq_len^2\n",
    "print(torch.allclose(S.grad ,dS))\n",
    "dQ = dS @ K  # seq_len x seq_len @ seq_len x dim  flops--> seq_len^2 x dim\n",
    "print(torch.allclose(Q.grad ,dQ))\n",
    "dK = dS.T @ Q # seq_len x dim @ dim x seq_len  flops--> seq_len^2 x dim\n",
    "print(torch.allclose(K.grad ,dK))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "i = (0,1) # 0~seq_len-1\n",
    "j = (0,1) # 0~seq_len-1\n",
    "o_i =  O_ori[i,:]\n",
    "p_i = P[i,:]\n",
    "do_i = dO[i,:] \n",
    "q_i = Q[i,:]\n",
    "k_i = K[j,:]\n",
    "v_i = V[j,:]\n",
    "s_ij = S[i,j]\n",
    "# print(do_i.shape,V.shape)\n",
    "dp_i = do_i @ v_i.T\n",
    "print(torch.allclose(dp_i,P.grad[i,j]))\n",
    "# # D_i =  (o_i @ do_i.T) # 此处是错误的，需要把对角线元素取出来\n",
    "# D_i = (o_i * do_i).sum(dim = -1)\n",
    "# print(do_i.shape,dp_i.shape,D_i.shape)\n",
    "# ds_i = p_i*dp_i - D_i[:,None]*p_i\n",
    "# torch.allclose(ds_i,S.grad[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------------------------------------------\n",
    "# i = (0,1) # 0~seq_len-1\n",
    "# o_i =  O_ori[i,:]\n",
    "# p_i = P[i,:]\n",
    "# do_i = dO[i,:] \n",
    "# q_i = Q[i,:]\n",
    "# k_i = K[i,:]\n",
    "# v_i = V[i,:]\n",
    "# # print(do_i.shape,V.shape)\n",
    "# dp_i = do_i @ V.T\n",
    "# print(torch.allclose(dp_i,P.grad[i,:]))\n",
    "# # D_i =  (o_i @ do_i.T) # 此处是错误的，需要把对角线元素取出来\n",
    "# D_i = (o_i * do_i).sum(dim = -1)\n",
    "# print(do_i.shape,dp_i.shape,D_i.shape)\n",
    "# ds_i = p_i*dp_i - D_i[:,None]*p_i\n",
    "# torch.allclose(ds_i,S.grad[i,:])\n",
    "\n",
    "\n",
    "\n",
    "q_i = Q[i,:]\n",
    "dq_i = ds_i @ K\n",
    "print(torch.allclose(dq_i,Q.grad[i,:]))\n",
    "dK = ds_i.T @ q_i\n",
    "torch.allclose(dK,K.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "diag(): Supports 1D or 2D tensors. Got 3D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn((\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: diag(): Supports 1D or 2D tensors. Got 3D"
     ]
    }
   ],
   "source": [
    "x = torch.randn((5,2,2))\n",
    "torch.diag(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
